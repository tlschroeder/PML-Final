{
  "name": "Pml-final",
  "tagline": "",
  "body": "---\r\ntitle: \"Practical Machine Learning Final\"\r\noutput: html_document\r\n---\r\n\r\n```{r setup, include=FALSE}\r\nknitr::opts_chunk$set(echo = TRUE)\r\n```\r\n\r\n## Introduction\r\nIn the course of this assignment, we will use R to create a machine learning algorithm designed to take input from a series of accelerometers and determine in which of 5 ways a lifting exercise is being performed. First, we must load the data into R and examine its basic features:\r\n```{r}\r\nlibrary(caret)\r\ntest = read.csv(\"pml-testing.csv\")\r\ntrain = read.csv(\"pml-training.csv\")\r\nncol(train)\r\nnrow(train)\r\nnrow(test)\r\n```\r\n\r\nAs expected, our training set is significantly larger than our test set. While it was omitted in the above code for readability purposes, we also call head() on our dataset to take a quick look at the variables. To improve our accuracy, we will remove those with insignificant variance. We will also remove the column X, as it is merely an index and contains no usable information. While we're at it, we'll create a version of the test set with the target variable removed:\r\n\r\n```{r}\r\ncutcol = nearZeroVar(train, saveMetrics = TRUE)\r\ntrain = train[,cutcol$nzv == FALSE]\r\ntest = test[,cutcol$nzv == FALSE]\r\ntrain$X = NULL\r\ntest$X = NULL\r\ntest$user_name = NULL\r\ntrain$user_name = NULL\r\n```\r\nHowever, this still leaves us with a number of NAs in our data, which is problematic for many analyses. Examination of the dataset reveals that NAs are heavily clustered in a few columns and can be safely removed:\r\n\r\n```{r}\r\ntrain = train[,colMeans(is.na(train)) == 0]\r\ntest = test[,colMeans(is.na(test)) == 0]\r\n```\r\n\r\n\r\nIn order to avoid overfitting to the test set, we will set up cross validation using a 70-30 split of our training set:\r\n```{r}\r\nset.seed = 1\r\nsplit = createDataPartition(train$classe, p = 0.7, list = FALSE)\r\ncrosstrain = train[split,]\r\ncrossval = train[-split,]\r\n```\r\n\r\nFinally, we can begin to construct our model. For this assignment, we will be using a random forest to interpret the data. We use the training portion of our partitioned dataset, check the results using the cross validation portion, and examine the accuracy using confusionMatrix:\r\n```{r}\r\nlibrary(randomForest)\r\nlibrary(rpart)\r\nforest = randomForest(classe ~ ., data=crosstrain, method=\"class\")\r\ncrosspred = predict(forest, crossval, type = \"class\")\r\nconfusionMatrix(crosspred, crossval$classe)\r\n```\r\n\r\nOur accuracy is above 99%, which is very good! Note specifically that this is a cross validation test and not merely application to the existing data, so we also anticipate an out-of-sample error rate of less than 1%. Since our cross validation has yielded such positive results, we will use a random forest to generate our final outcome using the full training set and apply it to the test set. Initial attempts produced errors relating to predictors not matching; these are addressed by quickly cleaning up factor levels before creating the full forest.\r\n\r\nAll that's left is to run our analysis:\r\n\r\n```{r}\r\ncommon = intersect(names(train), names(test)) \r\nfor (p in common) { \r\n  if (class(train[[p]]) == \"factor\") { \r\n    levels(test[[p]]) = levels(train[[p]]) \r\n  } \r\n}\r\n\r\n\r\nfinalForest = randomForest(classe ~ ., data=train, method=\"class\")\r\ntest$problem_id = as.factor(test$problem_id)\r\npredict(forest, newdata = test, type = \"class\")\r\n```\r\n\r\n...Which produces a tidy list of our predictions for this assignment.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}